import streamlit as st
from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
import openai
from fetch_news import fetch_naver_news
import os
from dotenv import load_dotenv
from datetime import datetime
from bs4 import BeautifulSoup
import re
import spacy
from spacy.matcher import PhraseMatcher

load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if OPENAI_API_KEY:
    print("API í‚¤ ë¡œë“œ ì„±ê³µ")
else:
    print("API í‚¤ ë¡œë“œ ì‹¤íŒ¨")

embedding_model = OpenAIEmbeddings(model="text-embedding-ada-002", api_key=OPENAI_API_KEY)
llm = ChatOpenAI(model="gpt-4o-mini", api_key=OPENAI_API_KEY)

persist_directory = "./chroma_db"

# Streamlit
st.title("ê¸ˆìœµ ë‰´ìŠ¤ ìš”ì•½ ì±—ë´‡ ğŸ’¬")
st.write("ì±—ë´‡ì— ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´ ìµœì‹  ê¸ˆìœµ ë‰´ìŠ¤ë¥¼ ìš”ì•½í•˜ì—¬ ë‹µë³€í•´ë“œë¦½ë‹ˆë‹¤!")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state["messages"] = []

# ìµœì´ˆ ì ‘ì† ì‹œ ì¸ì‚¬ë§
if len(st.session_state.messages) == 0:
    st.session_state["messages"].append({"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”! ì§ˆë¬¸ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”."})

# ì´ì „ ë©”ì‹œì§€ ì¶œë ¥
for msg in st.session_state.messages:
    st.chat_message(msg['role']).write(msg['content'])

# ì‚¬ìš©ì ì…ë ¥
user_input = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”ğŸ™‚(ex.2025ë…„ 1ì›” 13ì¼ì˜ ê¸ˆìœµ ë‰´ìŠ¤ë¥¼ ì•Œë ¤ì¤„ë˜?, ìµœì‹  ê¸ˆìœµ ë™í–¥ì„ ì•Œë ¤ì¤˜, ...)")

# spaCy ëª¨ë¸ ë¡œë“œ (í•œêµ­ì–´)
nlp = spacy.load("ko_core_news_sm")

# íŠ¹ì • í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
predefined_keywords = [
    'Tesla', 'Bitcoin', 'Stock', 'ê²½ì œ', 'ê¸ˆìœµ', 'ì£¼ì‹', 'í…ŒìŠ¬ë¼', 'ë¹„íŠ¸ì½”ì¸', 
    'ê¸ˆë¦¬', 'ì±„ê¶Œ', 'ì£¼ì‹ì‹œì¥', 'ì¦ê¶Œ', 'ì£¼ì‹ê±°ë˜', 'ETF', 'í¬íŠ¸í´ë¦¬ì˜¤', 'í€ë“œ', 
    'ì£¼ì‹íˆ¬ì', 'ê°€ìƒí™”í', 'í¬ë¦½í† ', 'ë¸”ë¡ì²´ì¸', 'ìƒì¥', 'ì½”ìŠ¤í”¼', 'ì½”ìŠ¤ë‹¥', 
    'ìƒì¥íì§€', 'ìƒì¥ì£¼ì‹', 'ë°°ë‹¹', 'ì£¼ì‹ë°°ë‹¹', 'ì‹œê°€ì´ì•¡', 'ì´ììœ¨', 'ìì‚°', 
    'ìì‚°ìš´ìš©', 'ë¦¬ìŠ¤í¬ê´€ë¦¬', 'ì±„ê¶Œì‹œì¥', 'í—¤ì§€í€ë“œ', 'íˆ¬ìì „ëµ', 'ê²½ì œì§€í‘œ', 
    'ë¬¼ê°€', 'ì†Œë¹„ìë¬¼ê°€', 'í™˜ìœ¨', 'ê¸ˆìœµìœ„ê¸°', 'ê¸ˆìœµì •ì±…', 'ê¸ˆìœµì‹œìŠ¤í…œ', 'êµ­ì±„', 
    'ì§€ìˆ˜', 'ìƒìŠ¹ë¥ ', 'í•˜ë½ë¥ ', 'ì¦ì‹œ', 'ë§¤ìˆ˜', 'ë§¤ë„', 'ìì‚°ê´€ë¦¬', 'ê³ ë°°ë‹¹ì£¼', 
    'ê¸ˆìœµìƒí’ˆ', 'ë¶€ë™ì‚°', 'ëª¨ê¸°ì§€', 'ëŒ€ì¶œ', 'ì±„ê¶Œê¸ˆë¦¬', 'ê¸ˆìœµê¸°ê´€', 'ê±°ë˜ì†Œ', 
    'ë¦¬ë³´ê¸ˆë¦¬', 'ê¸ˆìœµê·œì œ', 'FOMC', 'IMF', 'OECD', 'GDP', 'ì‹¤ì—…ë¥ ', 'ì¸í”Œë ˆì´ì…˜', 
    'ìœ ë™ì„±', 'ë§ˆì§„', 'í—¤ì§€', 'ì˜µì…˜', 'ì„ ë¬¼', 'ì£¼ì‹ë¶„ì„', 'ê¸°ì—…ë¶„ì„', 'ë§¤ì¶œ', 'ìˆœì´ìµ', 
    'ì˜ì—…ì´ìµ', 'ë¹„ì¦ˆë‹ˆìŠ¤ëª¨ë¸', 'ê°€ì¹˜íˆ¬ì', 'ì„±ì¥íˆ¬ì', 'ì›ŒëŸ°ë²„í•', 'í…Œí¬ì£¼', 'ê·¸ë¡œì“°', 
    'ì¸ë±ìŠ¤í€ë“œ', 'íˆ¬ìì', 'ì£¼ìš”ì§€í‘œ', 'ì €ê¸ˆë¦¬', 'ê¸ˆìœµì‚¬ê¸°', 'í•€í…Œí¬', 'ëª¨ë°”ì¼ë±…í‚¹', 
    'ë””ì§€í„¸ìì‚°', 'í•€í…Œí¬ê¸°ì—…', 'ê¸ˆìœµê¸°ìˆ ', 'ë¸”ë¡ì²´ì¸ê¸°ìˆ ', 'ë””ì§€í„¸í™”í', 'ë¦¬ë¸Œë¼', 
    'ìŠ¤í…Œì´ë¸”ì½”ì¸', 'ëŒ€ì²´íˆ¬ì', 'ê²½ê¸°ì§€í‘œ', 'ì¦ê¶Œì‚¬', 'ê¸ˆìœµì»¨ì„¤íŒ…', 'ê³ ì •ê¸ˆë¦¬', 
    'ë³€ë™ê¸ˆë¦¬', 'êµ­ì œê¸ˆìœµ', 'ê¸ˆìœµë¶„ì„', 'ê²½ì œìœ„ê¸°', 'ê²½ì œì„±ì¥', 'ê³ ìš©ì§€í‘œ', 'ìƒì¥ê¸°ì—…', 
    'íˆ¬ìê¸°íšŒ', 'ì •ì±…ê¸ˆë¦¬', 'ê¸°ì¤€ê¸ˆë¦¬', 'ê¸ˆìœµê±°ë˜', 'ê°€ì¹˜ì£¼', 'ì„±ì¥ì£¼', 'ì‹ ìš©ì¹´ë“œ', 
    'í•´ì™¸ì£¼ì‹', 'ìë³¸ì‹œì¥', 'ì¤‘ì•™ì€í–‰', 'ê¸ˆìœµì—…ê³„', 'íšŒê³„ê¸°ì¤€', 'ê¸°ì—…íšŒê³„', 'íšŒê³„ì‚¬'
]


# ì‚¬ìš©ì ì…ë ¥ì—ì„œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
def extract_keyword(text):
    """ì‚¬ìš©ì ì…ë ¥ì—ì„œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    doc = nlp(text)
    
    # PhraseMatcherë¥¼ ì‚¬ìš©í•˜ì—¬ ë¯¸ë¦¬ ì •ì˜ëœ í‚¤ì›Œë“œë¥¼ ë¬¸ì¥ì—ì„œ ì°¾ê¸°
    matcher = PhraseMatcher(nlp.vocab)
    patterns = [nlp.make_doc(keyword) for keyword in predefined_keywords]
    matcher.add("PredefinedKeywords", patterns)

    matches = matcher(doc)
    matched_keywords = [doc[start:end].text for _, start, end in matches]

    # í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ì¶”ì¶œí•˜ê³ , ì—†ìœ¼ë©´ None ë°˜í™˜
    if matched_keywords:
        return matched_keywords[0]  # ì²«ë²ˆì§¸ í‚¤ì›Œë“œë§Œ ë°˜í™˜
    else:
        return None


# ë‚ ì§œ ì¶”ì¶œ í•¨ìˆ˜
def extract_date(text):
    """
    ì…ë ¥ëœ í…ìŠ¤íŠ¸ì—ì„œ ë‚ ì§œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    - "YYYYë…„ MMì›” DDì¼", "MMì›” DDì¼", "DDì¼" ë“±ì˜ ë‹¤ì–‘í•œ í˜•ì‹ ì§€ì›
    - í˜•ì‹ì´ ì—†ìœ¼ë©´ None ë°˜í™˜
    """
    today = datetime.today()
    
    # ì •ê·œ í‘œí˜„ì‹ íŒ¨í„´
    patterns = [
        (r"(\d{4})ë…„ (\d{1,2})ì›” (\d{1,2})ì¼", "%Yë…„ %mì›” %dì¼"),  # YYYYë…„ MMì›” DDì¼
        (r"(\d{1,2})ì›” (\d{1,2})ì¼", "%mì›” %dì¼"),                # MMì›” DDì¼
        (r"(\d{1,2})ì¼", "%dì¼")                                  # DDì¼
    ]
    
    for pattern, date_format in patterns:
        match = re.search(pattern, text)
        if match:
            try:
                if date_format == "%dì¼":  # ë‚ ì§œë§Œ ì£¼ì–´ì§„ ê²½ìš°
                    day = int(match.group(1))
                    return today.replace(day=day)  # ì´ë²ˆ ë‹¬ì˜ í•´ë‹¹ ë‚ ì§œ ë°˜í™˜
                elif date_format == "%mì›” %dì¼":  # ì›”ê³¼ ë‚ ì§œë§Œ ì£¼ì–´ì§„ ê²½ìš°
                    month, day = map(int, match.groups())
                    return today.replace(month=month, day=day)
                else:  # YYYYë…„ MMì›” DDì¼ í˜•ì‹
                    return datetime.strptime(match.group(), date_format)
            except ValueError:
                # ìœ íš¨í•˜ì§€ ì•Šì€ ë‚ ì§œì¼ ê²½ìš° ë¬´ì‹œ
                continue
    
    return None


# HTML íƒœê·¸ë¥¼ ì œê±°í•˜ëŠ” í•¨ìˆ˜
def clean_html(text):
    """HTML íƒœê·¸ë¥¼ ì œê±°í•©ë‹ˆë‹¤."""
    soup = BeautifulSoup(text, "html.parser")
    return soup.get_text()


# ê¸°ì‚¬ ë‚´ìš©ì„ ì²­í¬ë¡œ ë‚˜ëˆ„ëŠ” í•¨ìˆ˜
def chunk_text(text, chunk_size=1000):
    """ê¸°ì‚¬ ë‚´ìš©ì„ 1000ì ì´í•˜ë¡œ ì²­í¬ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤."""
    chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]
    return chunks


if user_input:
    st.session_state.messages.append({"role": "user", "content": user_input})

    # ë‚ ì§œì™€ í‚¤ì›Œë“œ ì¶”ì¶œ
    date = extract_date(user_input)
    keyword = extract_keyword(user_input)

    # ê¸°ë³¸ ì¿¼ë¦¬
    query = user_input if not keyword else keyword

    # ë‚ ì§œê°€ ìˆìœ¼ë©´ ì¿¼ë¦¬ì— í¬í•¨
    if date:
        query = f"{query} {date.strftime('%Y-%m-%d')}" # ë‚ ì§œë¥¼ ì¿¼ë¦¬ì— í¬í•¨

    with st.spinner("ë„¤ì´ë²„ì—ì„œ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ëŠ” ì¤‘..."):
        try:
            # ë‰´ìŠ¤ ê²€ìƒ‰
            news_items = fetch_naver_news(query, display=20)

            # ë‚ ì§œ í•„í„°ë§ (ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë‚ ì§œì— í•´ë‹¹í•˜ëŠ” ê¸°ì‚¬ë§Œ ì„ íƒ)
            filtered_news = []
            if date:
                for item in news_items:
                    pub_date = datetime.strptime(item["pubDate"], "%a, %d %b %Y %H:%M:%S %z")
                    if pub_date.date() == date.date():
                        filtered_news.append(item)
            else:
                filtered_news = news_items

            if not filtered_news:
                # st.warning("ì§€ì •ëœ ë‚ ì§œë‚˜ ì¿¼ë¦¬ì— ë§ëŠ” ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
                st.session_state.messages.append({"role": "assistant", "content": "ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤ğŸ¥²"})
            else:
                # ë‰´ìŠ¤ ë‚´ìš© ì „ì²˜ë¦¬ ë° ë²¡í„°í™”
                documents = []
                summaries = []  # ìš”ì•½ ë¦¬ìŠ¤íŠ¸
                sources = []  # ì¶œì²˜ ë¦¬ìŠ¤íŠ¸
                titles = []  # ì œëª© ë¦¬ìŠ¤íŠ¸
                dates = []  # ë‚ ì§œ ë¦¬ìŠ¤íŠ¸

                for item in filtered_news:
                    # HTML íƒœê·¸ ì œê±°
                    clean_article = clean_html(item["description"])
                    
                    raw_title = clean_html(item["title"])
                    clean_title = re.sub(r"\[.*?\]", "", raw_title).strip()

                    chunks = chunk_text(clean_article)
                    for chunk in chunks:
                        documents.append({"content": chunk, "source": item["originallink"]})
                    
                    # ìš”ì•½
                    summaries.append(clean_html(item["description"]))

                    # ì¶œì²˜
                    sources.append(item["originallink"])

                    # ì œëª©
                    titles.append(clean_title)

                    # ë‚ ì§œ
                    pub_date = datetime.strptime(item["pubDate"], "%a, %d %b %Y %H:%M:%S %z")
                    formatted_date = pub_date.strftime('%Y-%m-%d %p %Iì‹œ')
                    formatted_date = formatted_date.replace('AM', 'ì˜¤ì „').replace('PM', 'ì˜¤í›„')
                    dates.append(formatted_date)

                texts = [doc["content"] for doc in documents]
                metadatas = [{"source": doc["source"]} for doc in documents]

                # Chroma ë²¡í„°ìŠ¤í† ì–´ ìƒì„±, persist_directory ê²½ë¡œ ì„¤ì •
                vectorstore = Chroma.from_texts(
                    texts, 
                    embedding_model, 
                    persist_directory=persist_directory, 
                    metadatas=metadatas
                )
                retriever = vectorstore.as_retriever()

                # QA ì²´ì¸ ì„¤ì •
                qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, return_source_documents=True)

                # ì¿¼ë¦¬ ì²˜ë¦¬
                result = qa_chain(query)
                answer = result["result"]

                # ì±—ë´‡ í˜•ì‹ì˜ ì‘ë‹µ ì¶œë ¥
                st.session_state.messages.append({"role": "assistant", "content": answer})

                # ì œëª©, ë‚ ì§œ, ìš”ì•½, ì¶œì²˜ ì¶œë ¥
                for title, summary, source, date in zip(titles, summaries, sources, dates):
                    combined_message = f"""
                    <div style="background-color:#FFFFFF; padding:15px; border-radius:10px; margin-bottom:20px; border:1px solid #ddd; width: 100%;">
                        <h5 style="color:#333; font-weight:bold; margin-bottom:10px; white-space: normal; word-wrap: break-word; overflow-wrap: break-word;">{title}</h5>
                        <p style="font-size:14px; color:#888; margin-bottom:10px;"><strong>ë‚ ì§œ:</strong> {date}</p>
                        <p style="font-size:16px; color:#444; margin-bottom:15px;">{summary}</p>
                        <div style="text-align:right;">
                            <a href="{source}" target="_blank" style="display:inline-block; background-color:#FFBF00; color:white; padding:5px 15px; border-radius:5px; text-decoration:none;">ğŸ”— ì›ë¬¸ ë³´ê¸°</a>
                        </div>
                    </div>
                    """

                    st.session_state.messages.append({"role": "assistant", "content": combined_message})

        except Exception as e:
            st.error(f"ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

# ì „ì²´ ìš”ì•½ ì¶œë ¥
for msg in st.session_state.messages:
    if msg['role'] == 'assistant':
        st.markdown(msg['content'], unsafe_allow_html=True)
    else:
        st.chat_message(msg['role']).write(msg['content'])